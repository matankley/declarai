{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Declarai","text":"<p> Declarai, turning Python code into LLM tasks, easy to use, and production-ready. </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Declarai turns your Python code into LLM tasks, utilizing python's native syntax, like type hints and docstrings, to instruct an AI model on what to do.</p> <p>Designed with a clear focus on developer experience, you simply write Python code as you normally would, and Declarai handles the rest.</p> <p>poem_generator.py<pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.task\ndef generate_poem(title: str) -&gt; str:\n\"\"\"\n    Write a 4 line poem on the provided title\n    \"\"\"\n\n\nres = generate_poem(\n    title=\"Declarai, the declarative framework for LLMs\"\n)\nprint(res)\n</code></pre> <pre><code>&gt;&gt;&gt; Declarai, the AI framework,\n... Empowers LLMs with declarative power,\n... Efficiently transforming data and knowledge,\n... Unlocking insights in every hour.\n</code></pre></p>"},{"location":"#why-use-declarai","title":"Why use Declarai?","text":"<ul> <li> <p>Pythonic Interface to LLM: - Leverage your existing Python skills instead of spending unnecessary time creating complex prompts.</p> </li> <li> <p>Lightweight: - Declarai is written almost solely in python 3.6 using only pydantic and openai SDKs, so there's no need to worry about dependency spaghetti.</p> </li> <li> <p>Extendable: - Declarai is designed to be easily extendable, the interface is simple and accessible by design so   you can easily override or customize the behavior of the framework to your specific needs.</p> </li> <li> <p>Type-Driven Prompt Design: - Through the application of Python's type annotations,    Declarai constructs detailed prompts that guide Large Language Models (LLMs) to generate the desired output type.</p> </li> <li> <p>Context-Informed Prompts via Docstrings: - Implement function docstrings to supply contextual data to LLMs,    augmenting their comprehension of the designated task, thereby boosting their performance.</p> </li> <li> <p>Automated Execution of LLM Tasks: - Declarai takes care of the execution code, letting you concentrate on the core business logic.</p> </li> </ul> <p>Utilizing Declarai leads to improved code readability, maintainability, and predictability.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install declarai\n\n---&gt; 100%\nDone!\n</code></pre>"},{"location":"#feature-highlight","title":"Feature highlight","text":""},{"location":"#python-native-syntax","title":"Python native syntax","text":"<p>Integrating deeply into python's native syntax, declarai understands your code and generates the prompt accordingly.</p> Simple Syntax<pre><code>@declarai.task # (1)!\ndef rank_by_severity(message: str) -&gt; int: # (2)!\n\"\"\"\n    Rank the severity of the provided message by it's urgency.\n    Urgency is ranked on a scale of 1-5, with 5 being the most urgent.\n    :param message: The message to rank\n    :return: The urgency of the message\n    \"\"\" # (3)!\n\n\nprint(rank_by_severity(message=\"The server is down!\"))\n#&gt; 5\nprint(rank_by_severity(message=\"How was your weekend?\"))\n#&gt; 1\n</code></pre> <ol> <li>The <code>@declarai.task</code> decorator marks the function as a Declarai prompt task.</li> <li>The type hints <code>List[str]</code> are used to parse the output of the llm into a list of strings.</li> <li>The docstring represents the task's description which is used to generate the prompt.<ul> <li><code>description</code> - the context of the task</li> <li><code>:param</code> - The function's parameters and their description</li> <li><code>:return</code> - The output description</li> </ul> </li> </ol>"},{"location":"#support-python-typing-and-pydantic-models","title":"Support Python typing and pydantic models","text":"<p>Declarai will return a serialized object as defined by the type hints at runtime. Builtins<pre><code>@declarai.task\ndef extract_phone_number(email_content: str) -&gt; List[str]:\n\"\"\"\n    Extract the phone numbers from the provided email_content\n    :param email_content: Text that represents the email content \n    :return: The phone numbers that are used in the email\n    \"\"\"\n\nprint(extract_phone_number(email_content=\"Hi, my phone number is 123-456-7890\"))\n#&gt; ['123-456-7890']\n</code></pre></p> Builtins<pre><code>@declarai.task\ndef datetime_parser(raw_date: str) -&gt; datetime:\n\"\"\"\n    Parse the input into a valid datetime string of the format YYYY-mm-ddThh:mm:ss\n    :param raw_date: The provided raw date\n    :return: The parsed datetime output\n    \"\"\"\n\n\nprint(datetime_parser(raw_date=\"Janury 1st 2020\"))\n#&gt; 2020-01-01 00:00:00\n</code></pre> <p>Pydantic models<pre><code>class Animal(BaseModel):\n    name: str\n    family: str\n    leg_count: int\n\n\n@declarai.task\ndef suggest_animals(location: str) -&gt; Dict[int, List[Animal]]:\n\"\"\"\n    Create a list of numbers from 0 to 5\n    for each number, suggest a list of animals with that number of legs\n    :param location: The location where the animals can be found\n    :return: A list of animal leg count and for each count, the corresponding animals\n    \"\"\"\n\n\nprint(suggest_animals(location=\"jungle\"))\n#&gt; {\n#       0: [\n#           Animal(name='snake', family='reptile', leg_count=0)\n#       ], \n#       2: [\n#           Animal(name='monkey', family='mammal', leg_count=2), \n#           Animal(name='parrot', family='bird', leg_count=2)\n#       ], \n#       4: [\n#          Animal(name='tiger', family='mammal', leg_count=4), \n#          Animal(name='elephant', family='mammal', leg_count=4)\n#       ]\n# }\n</code></pre> </p>"},{"location":"#chat-interface","title":"Chat interface","text":"<p>Create chat interfaces with ease, simply by writing a class with docstrings</p> <p>Info</p> <p>Notice that <code>chat</code> is exposed under the <code>experimental</code> namespace, noting this interface is still work in progress.</p> <p><pre><code>@declarai.experimental.chat\nclass CalculatorBot:\n\"\"\"\n    You a calculator bot,\n    given a request, you will return the result of the calculation\n    \"\"\"\n\n    def send(self, message: str) -&gt; int: ...\n\n\ncalc_bot = CalculatorBot()\nprint(calc_bot.send(message=\"1 + 1\"))\n#&gt; 2\n</code></pre> </p>"},{"location":"#task-middlewares","title":"Task Middlewares","text":"<p>Easy to use middlewares provided out of the box as well as the ability to easily create your own.</p> <p>Logging Middleware<pre><code>@declarai.task(middlewares=[LoggingMiddleware])\ndef extract_info(text: str) -&gt; Dict[str, str]:\n\"\"\"\n    Extract the phone number, name and email from the provided text\n    :param text: content to extract the info from\n    :return: The info extracted from the text\n    \"\"\"\n    return Declarai.magic(text=text)\n\nres = extract_info(\n    text=\"Hey jenny,\"\n    \"you can call me at 124-3435-132.\"\n    \"You can also email me at georgia@coolmail.com\"\n    \"Have a great week!\"\n)\nprint(res)\n</code></pre> Result: <pre><code>{'task_name': 'extract_info', 'llm_model': 'gpt-3.5-turbo-0301', 'template': '{input_instructions}\\n{input_placeholder}\\n{output_instructions}', 'template_args': {'input_instructions': 'Extract the phone number, name and email from the provided text', 'input_placeholder': 'Inputs:\\ntext: {text}\\n', 'output_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\'```json\\' and \\'```\\':\\n```json\\n{{\\n    \"declarai_result\": Dict[str, str]  # The info extracted from the text\\n}}\\n```'}, 'prompt_config': {'structured': True, 'multi_results': False, 'return_name': 'declarai_result', 'temperature': 0.0, 'max_tokens': 2000, 'top_p': 1.0, 'frequency_penalty': 0, 'presence_penalty': 0}, 'call_kwargs': {'text': 'Hey jenny,you can call me at 124-3435-132.You can also email me at georgia@coolmail.comHave a great week!'}, 'result': {'phone_number': '124-3435-132', 'name': 'jenny', 'email': 'georgia@coolmail.com'}, 'time': 2.192906141281128}\n{'phone_number': '124-3435-132', 'name': 'jenny', 'email': 'georgia@coolmail.com'}\n</code></pre></p> <p>We highly recommend you to go through the beginner's guide to get a better understanding of the library and its capabilities - Beginner's Guide</p>"},{"location":"src/changelog/","title":"Changelog","text":""},{"location":"src/changelog/#v012","title":"v0.1.2","text":"<p>View full release on GitHub and PyPi</p> <p>Minor bug fixes</p> <p>Changes:</p> <ul> <li>Updates to documentation</li> <li>Updates to dependencies with reported vulnerabilities</li> <li>Fix typing and improve support for IDE autocompletion</li> <li>Fix issue with initialization failing when passed the <code>openai_token</code> at runtime.</li> </ul>"},{"location":"src/changelog/#v011","title":"v0.1.1","text":"<p>View full release on GitHub and PyPi</p> <p>Announcing the first release of Declarai! \ud83e\udd73 \ud83e\udd73</p> <p>Declarai was born out of the awe and excitement of LLMs, along with our passion for excellent engineering and real-world applications at scale.</p> <p>We hope this project will help introduce more developers into the world of LLMs and enable them to more easily and reliably integrate these amazing capabilities into their production systems.</p> <p>Main features:</p> <ul> <li>Task interface</li> <li>Chat interface</li> <li>Middlewares</li> <li>Exhaustive documentation</li> <li>OpenAI support</li> <li>LLM prompt best practices</li> </ul>"},{"location":"src/contribute/","title":"Contribute","text":"<p>Do you like Declarai?</p> <p>Spread the word!</p> <ul> <li>Star  the repository</li> <li>Share the link to the repository with your friends and colleagues</li> <li>Watch the github repository to get notified about new releases.</li> </ul>"},{"location":"src/contribute/#development","title":"Development","text":"<p>Once you have cloned the repository, install the requirements:</p> <p>Using <code>venv</code></p> PoetryVenv <pre><code>poetry install\n</code></pre> <pre><code>python -m venv env\nsource env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n</code></pre>"},{"location":"src/contribute/#documentation","title":"Documentation","text":"<p>The documentation is built using MkDocs. To view the documentation locally, run the following command:</p> <pre><code>$ cd docs\n$ mkdocs serve\nINFO    -  [11:37:30] Serving on http://127.0.0.1:8000/\n</code></pre>"},{"location":"src/contribute/#testing","title":"Testing","text":"<p>The testing framework used is pytest. To run the tests, run the following command:</p> <pre><code>pytest --cov=src   </code></pre>"},{"location":"src/contribute/#pull-requests","title":"Pull Requests","text":"<p>It should be extermly easy to contribute to this project. If you have any ideas, just open an pull request and we will discuss it.</p> <pre><code>git checkout -b my-new-feature\ngit commit -am 'Add some feature'\ngit push origin my-new-feature\n</code></pre>"},{"location":"src/beginners-guide/","title":"Tutorial - Beginners guide","text":"<p>This tutorial is a step-by-step guide to using Declarai. It walks you through the most basic features of the library.</p> <p>Each section gradually builds on the previous one while sections are structured by topic,  so that you can skip to whichever part is relevant to you. </p>"},{"location":"src/beginners-guide/#before-we-start","title":"Before we start","text":"<p>If you haven't already, install the Declarai library as follows:</p> <pre><code>$ pip install declarai\n</code></pre> <p>Info</p> <p>For this tutorial you will need an openai token. This token is completely your's and is not shared, stored or managed anywhere but on your machine! you can see more information about obtaining a token here: openai</p> <p>After installation, open a python file and start with setting up your declarai app:</p> <p>Once completed, the rest of the examples in this module should be as simple as copy/paste.</p> declarai_tutorial.py<pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\", openai_token=\"&lt;your-openai-token&gt;\")\n</code></pre> <p>Info</p> <p>Do your best to copy, run and edit the code in your editor to really understand how powerful Declarai is.</p>          Lets go!"},{"location":"src/beginners-guide/#advanced","title":"Advanced","text":"<p>If you feel this tutorial is too easy, feel free to jump to our Features section, which covers more complex  topics like middlewares, running evaluations and building multi provider flows.</p> <p>We recommend you read the tutorial first, and then the advanced guide if you want to learn more.</p>"},{"location":"src/beginners-guide/controlling-task-behavior/","title":"Controlling task behavior","text":"<p>Task behavior can be controlled by any of the available interfaces in Python. Controlling these parameters is key to achieving the desired results from the model.</p>"},{"location":"src/beginners-guide/controlling-task-behavior/#passing-parameters-to-the-task","title":"Passing parameters to the task","text":"<p>In the following example, we'll create a task that suggests movies to watch based on a given input. <pre><code>@declarai.task\ndef movie_recommender(user_input: str): # (1)!\n\"\"\"\n    Recommend a movie to watch based on the user input\n    :param user_input: The user's input\n    \"\"\" # (2)!\n</code></pre></p> <ol> <li>Notice how providing a type hint for the <code>user_input</code> parameter allows declarai to understand the expected input type.</li> <li>Adding the param to the docstring allows declarai to communicate the meaning of this parameter to the model.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; 'Interstellar'\n</code></pre>"},{"location":"src/beginners-guide/controlling-task-behavior/#using-return-types-to-control-the-output","title":"Using return types to control the output","text":"<p>This is a good start,  but let's say we want to have a selection of movies instead of a single suggestion. <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; List[str]: # (1)!\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    :param user_input: The user's input\n    :return: A list of movie recommendations\n    \"\"\" # (2)!\n</code></pre></p> <ol> <li>Adding a return type hint allows declarai to parse the output of the llm into the provided type,     in our case a list of strings.</li> <li>Explaining the return value aids the model in returning the expected output and avoiding hallucinations.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; ['Interstellar', 'Gravity', 'The Martian', 'Apollo 13', '2001: A Space Odyssey', 'Moon', 'Sunshine', 'Contact', 'The Right Stuff', 'Hidden Figures']\n</code></pre> <p>Info</p> <p>Notice How the text in our documentation has changed from singular to plural form.  Maintaining consistency between the task's description and the return type is important for the model to understand the expected output. For more best-practices, see here. </p> <p>Awesome! </p> <p>Now we have a list of movies to choose from!</p> <p>But what if we want to go even further ?  Let's say we want the model to also provide a short description of each movie. <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]: # (1)!\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\" # (2)!\n</code></pre></p> <ol> <li>We've updated the return value to allow for the creation of a dictionary of movie names and descriptions.</li> <li>We re-enforce the description of the return value to ensure the model understands the expected output.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt;{\n   'Interstellar': \"A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\", \n   'Gravity': 'Two astronauts work together to survive after an accident leaves them stranded in space.', \n   'The Martian': 'An astronaut is left behind on Mars after his team assumes he is dead and must find a way to survive and signal for rescue.', \n   'Apollo 13': 'The true story of the Apollo 13 mission, where an explosion in space jeopardizes the lives of the crew and their safe return to Earth.', \n   '2001: A Space Odyssey': \"A journey through human evolution and the discovery of a mysterious black monolith that may hold the key to humanity's future.\"\n}\n</code></pre> <p>Info</p> <p>A good practice for code readability as well as great performing models is to use type hints and context in the docstrings. The better you describe the task, <code>:params</code> and <code>:return</code> sections within the docstring, the better the results will be.</p> <p>Tip</p> <p>Try experimenting with various descriptions and see how far you can push the model's understanding! who knows what you'll find !</p>          Previous           Next"},{"location":"src/beginners-guide/debugging-tasks/","title":"Debugging tasks","text":"<p>So it all seems pretty magical up to this point, but what if you want to see what's going on behind the scenes? Being able to debug your tasks is a very important part of the development process, and Declarai makes it easy for you.</p>"},{"location":"src/beginners-guide/debugging-tasks/#compiling-tasks","title":"Compiling tasks","text":"<p>The first and simplest tool to better understand what's happening under the hood is the <code>compile</code> method. Declarai has an <code>evals</code> module as well for advanced debugging and benchmarking which you can review later here: evals</p> <p>Let's take the last task from the previous section and add a call to the <code>compile</code> method: <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"\n</code></pre> <pre><code>print(movie_recommender.compile())\n\n&gt; {\n    'messages': [ # (1)!\n        # (2)!        \n        system: You are a REST api endpoint. \n                You only answer in JSON structures with a single key named 'declarai_result', nothing else. \n                The expected format is: \"declarai_result\": Dict[string, string]  # A dictionary of movie names and descriptions,\n        # (3)!\n        user: Recommend a selection of movies to watch based on the user input  \n              For each movie provide a short description as well.\n              Inputs: user_input: {user_input} # (4)!\n]}\n</code></pre></p> <ol> <li>As we are working with the openai llm provider, which exposes a chat interface, we translate the task into messages as defined by openai's API.</li> <li>In order to guide the task with the correct output format, we provide a system message that explains LLM's role and expected responses</li> <li>The user message is the actual translation of the task at hand, with the user's input as a placeholder for the actual value.</li> <li>{user_input} will be populated with the actual value when the task is being called at runtime.</li> </ol> <p>What we're seeing here is the template for this specific task. It is built so that when called at runtime,  it will be populated with the real values passed to our task.</p> <p>Warning</p> <p>As you can see, that the actual prompt being sent to the model is a bit different than the original docstring.  Even though Declarai incorporates best practices for prompt engineering while maintaining as little interference as possible with user prompts,   it is still possible that the model will not generate the desired output. For this reason it is important to be able to debug your tasks and understand what actually got sent to the model</p>"},{"location":"src/beginners-guide/debugging-tasks/#compiling-tasks-with-real-values","title":"Compiling tasks with real values","text":"<p>The <code>compile</code> method can also be used to view the prompt with the real values provided to the task. This is useful when prompts might behave differently for different inputs.</p> <pre><code>print(movie_recommender.compile())\n\n&gt; {\n    'messages': [     \n        system: You are a REST api endpoint. \n                You only answer in JSON structures with a single key named 'declarai_result', nothing else. \n                The expected format is: \"declarai_result\": Dict[string, string]  # A dictionary of movie names and descriptions,\n        user: Recommend a selection of movies to watch based on the user input  \n              For each movie provide a short description as well.\nInputs: user_input: I want to watch a movie about space # (1)!\n]}\n</code></pre> <ol> <li>The actual value of the parameter is now populated in the placeholder and we have our final prompt!</li> </ol> <p>Tip</p> <p>With the <code>compile</code> method, you can always take your prompts anywhere you like,   if it's for monitoring, debugging or just for documentation, we've got you covered!</p>          Previous           Next"},{"location":"src/beginners-guide/recap/","title":"Recap","text":"<p>In this tutorial you've covered the basics of Declarai! You should now be able to easily:</p> <ul> <li>Create a declarai task.</li> <li>control your task's behavior with native python</li> <li>Use the <code>compile</code> method to view and debug your task template and final prompt!</li> </ul>          Previous"},{"location":"src/beginners-guide/recap/#next-steps","title":"Next steps","text":"<p>You are welcome to explore our Features section, where you can find the full list of supported features and how to use them.</p>"},{"location":"src/beginners-guide/simple-task/","title":"Simple task","text":"<p>The simplest Declarai usage is a function decorated with <code>@declarai.task</code>:</p> <p><pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n\nprint(say_something())\n\n&gt; \"Spread love and kindness to make the world a better place.\"\n</code></pre> In Declarai, The docstring represents the task's description and is used to generate the prompt.</p> <p>By explaining what you want the task to do, the model will be able to understand it and reply with the proper result.</p>          Next"},{"location":"src/best-practices/","title":"Best practices","text":"<p>Prompt engineering is no simple task and there are various things to consider when creating a prompt. In this page we provide our view and understanding of the best practices for prompt engineering. These will help you create reliably performing tasks and chatbots that won't surprise you when deploying in production.</p> <p>Warning</p> <p>While this guide will should help in creating reliable prompts for most cases, it is still possible that the model will not generate the desired output.  For this reason we strongly recommend you test your tasks and bots on various inputs before deploying to production.  You can acheive this by writing integration tests or using our provided <code>evals</code> library to discover which models and wich  versions perform best for your specific use case.</p>"},{"location":"src/best-practices/#explicit-is-better-than-implicit","title":"Explicit is better than implicit","text":"<p>When creating a prompt, it is important to be as explicit as possible. Declarai provide various interfaces to provide context and guidance to the model.</p> <p>Reviewing the movie recommender example from the beginner's guide, we can see a collection of techniques to provide context to the model: <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"\n</code></pre></p> <p>Using type annotations in the input and output create predictability in software and enforce a strict interface with the model. The types are read and enforced by Declarai at runtime so that a produced result of the wrong type will raise an error instead of returned and causing unexpected behavior down the line.</p> <p>Docstrings are used to provide context to the model and to the user.</p> <ul> <li> <p>Task description - The first part of the docstring is the task itself, make sure to address the expected inputs and how to use them     You can implement various popular techniques into the prompt such as <code>few-shot</code>, which means providing example inputs and outputs for the model to learn from.</p> </li> <li> <p>Param descriptions - Explaining the meaning of the input parameters helps the model better perform with the provided inputs.     For example. when passing an argument called <code>input</code>, if you know that the expected input will be an email, or user message, it is best to explain this to the model.</p> </li> <li> <p>Return description - While typing are a great base layer for declaring the expected output,      explaining the exact structure and logic behind this structure will help the model better perform.     For example, given a return type of <code>Dict[str, str]</code>, explaining that this object will contain a mapping of movie names to their respective description      will help to model properly populate the resulting object.</p> </li> </ul>"},{"location":"src/best-practices/#language-consistency-and-ambiguity","title":"Language consistency and ambiguity","text":"<p>When providing prompts to the model, it is best practice to use language that correlates with the expected input and output. For example, in the following, the prompt is written in single form, while the resulting output is in plural form. (i.e. a list) <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; List[str]:\n\"\"\"\n    Recommend a movie to watch based on the user input\n    :param user_input: The user's input\n    :return: Recommended movie\n    \"\"\"\n</code></pre> This may easily confuse the model and cause it to produce unexpected results which will fail when parsing the results. Instead, we could write the prompt as follows: <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; List[str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    :param user_input: The user's input\n    :return: A list of recommended movies\n    \"\"\"\n</code></pre> This way it is clear to the model that we are expecting a list of movies and not a single movie.</p>"},{"location":"src/best-practices/#falling-back-to-string","title":"Falling back to string","text":"<p>In some cases, you might be working on a task or chat that has a mixture of behaviors that may not be consistent. For example in this implementation of a calculator bot, the bot usually returns numbers, but for the scenario that an error occurs, it returns a string. <pre><code>@declarai.experimental.chat\nclass CalculatorBot:\n\"\"\"\n    You a calculator bot,\n    given a request, you will return the result of the calculation\n    If you have a problem with the provided input, you should return an error explaining the problem.\n    For example, for the input: \"1 + a\" where 'a' is unknown to you, you should return: \"Unknown symbol 'a'\"\n    \"\"\"\n    def send(self, message: str) -&gt; Union[str, int]:\n        ...\n</code></pre> When using the created bot it should look like this: <pre><code>calc_bot = CalculatorBot()\nprint(calc_bot.send(message=\"1 + 3\"))\n#&gt; 4\nprint(calc_bot.send(message=\"34 * b\"))\n#&gt; Unknown symbol 'b'\n</code></pre> This way, instead of raising an error, the bot returns a string that explains the problem and allows the user to recover from the 'broken' state.</p>"},{"location":"src/features/","title":"Features","text":"<p>As Declarai is aimed at being completely extensible and configurable, we provide interfaces to override and interact with any of the default behaviours if you choose.</p> <p>We are still actively working on exposing all the necessary interfaces to make this possible, so if there are any interfaces you would like to see exposed, please vote or open an issue on our GitHub</p>"},{"location":"src/features/magic/","title":"Magic","text":"<p>The Magic callable is an \"empty\" function that can be used for two main scenarios:</p> <ul> <li>A placeholder for typing, so to simplify interaction with static typing without hacing to mark all Declarai functions with <code># type: ignore</code>:</li> <li>A replacement for the docstring content, if for some reason you don't want to use the docstring for the task description.</li> </ul>"},{"location":"src/features/magic/#magic-as-a-placeholder-for-typing","title":"Magic as a placeholder for typing","text":"<p>Without magic: <pre><code>@declarai.task\ndef suggest_nickname(real_name: str) -&gt; str: # (1)!\n\"\"\"\n    Suggest a nickname for a person\n    :param real_name: The person's real name\n    :return: A nickname for the person\n    \"\"\"\n</code></pre></p> <ol> <li>type hinter warning on unused argument <code>real_name</code> in function.</li> </ol> <p>with magic: <pre><code>@declarai.task\ndef suggest_nickname(real_name: str) -&gt; str:\n\"\"\"\n    Suggest a nickname for a person\n    :param real_name: The person's real name\n    :return: A nickname for the person\n    \"\"\"\n    return declarai.magic(real_name=real_name) # (1)!\n</code></pre></p> <ol> <li>type hint warning is resolved.</li> </ol>"},{"location":"src/features/magic/#replacement-for-docstring","title":"Replacement for docstring","text":"<p>In the scenario that you do not wan't to rely on the docstring for prompt generation, you can use the magic function to provide the description and parameters.</p> <pre><code>@declarai.task\ndef suggest_nickname(real_name: str) -&gt; str:\n    return declarai.magic(\n        real_name=real_name,\n        description=\"Suggest a nickname for a person\",\n        params={\"real_name\": \"The person's real name\"},\n        returns=\"A nickname for the person\",\n    )\n</code></pre> <p>This does take some of Declarai's magic out of the equation, but the result should be all the same.</p>"},{"location":"src/features/multi-model-multi-provider/","title":"Multiple models / Multiple providers","text":"<p>Declarai allows you to use multiple models from different providers in the same project. All you need to do is configure seperate Declarai instances for each model and provider.</p> <pre><code>from declarai import Declarai\n\n# Configure the first Declarai instance\ndeclarai_gpt35 = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n# Configure the second Declarai instance\ndeclarai_gpt4 = Declarai(provider=\"openai\", model=\"gpt-4\")\n\n# Now use the instances to create tasks\n@declarai_gpt35.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n\n@declarai_gpt4.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n</code></pre>"},{"location":"src/features/planning-future-tasks/","title":"Planning future tasks","text":""},{"location":"src/features/planning-future-tasks/#plan-task","title":"Plan task","text":"<p>Once you have defined your task, you can create a plan for it that is already populated with the real values of the parameters.</p> <p>The plan is an object you call and get the results. This is very helpful when you want to populate the task with the real values of the parameters but delay the execution of it. </p> <pre><code>from declarai import init_declarai, magic\n\ntask = init_declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@task\ndef say_something_about_movie(movie: str) -&gt; str:  \n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return magic(movie)\n\nplan = say_something_about_movie.plan(movie=\"Avengers\")\n\nprint(plan)\n&gt; #&lt;declarai.tasks.base_llm_task.LLMTaskFuture object at 0x106795790&gt;\n\n\n# Execute the task by calling the plan\nplan()\n&gt; ['I liked the action-packed storyline and the epic battle scenes.',\n   \"I didn't like the lack of character development for some of the Avengers.\"]\n</code></pre> <p>Important</p> <p>The plan is an object you call and get the results. This is very helpful when you want to populate the task with the real values of the parameters but delay the execution of it. If you just want to execute the task, you can call the task directly.</p> <pre><code>res = say_something_about_movie(movie=\"Avengers\")\n\n&gt; ['I liked the action-packed storyline and the epic battle scenes.',\n\"I didn't like the lack of character development for some of the Avengers.\"]\n</code></pre>"},{"location":"src/features/chat/","title":"Chatbots","text":"<p>Unlike tasks, chatbots are meant to keep the conversation going.  Instead of executing a single operation, they are built to manage conversation context over time.</p> <p>Declarai can be used to create chatbots. The simplest way to do this is to use the <code>@declarai.experimental.chat</code> decorator.</p> <p>We declare a \"system prompt\" in the docstring of the class definition. The system prompt is the initial command that instructs the bot on who they are and what's expected in the conversation. </p> <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions \n    \"\"\" # (1)!\n</code></pre> <ol> <li>The docstring represents the chatbot's description and is used to generate the prompt.</li> </ol> <pre><code>sql_bot = SQLBot()\nsql_bot.send(\"When should I use a LEFT JOIN?\") # (1)!\n\n&gt; \"You should use a LEFT JOIN when you want to return all rows from the left table, and the matched rows from the right table.\"\n</code></pre> <ol> <li>The created bot exposes a <code>send</code> method, by which you can interact and send messages.     Every call to send results with a response from the bot.</li> </ol> <p>Tip</p> <p>You can also declare the chatbot system prompt by doing the following <pre><code>@declarai.experimental.chat\nclass SQLBot:\n    pass\nsql_bot = SQLBot(system=\"You are a sql assistant. You help with SQL related questions with one-line answers.\")\n</code></pre></p>"},{"location":"src/features/chat/advanced-initialization/","title":"Initialization","text":"<p>Although using the docstring and class properties is the recommended way to initialize a chatbot, it is not the only way. In cases were relying on the class docstring and properties is problematic, we allow manually passing the chat arguments to the class constructor. This takes away from the magic that Declarai provides, but we are aware not everyone may be comfortable with it.</p>"},{"location":"src/features/chat/advanced-initialization/#initialization-by-passing-parameters","title":"Initialization by passing parameters","text":"<p>Let's see how we can initialize a chatbot by passing the <code>system</code> and <code>greeting</code> parameters as arguments.</p> <pre><code>@declarai.experimental.chat\nclass SQLBot:\n    ...\n\n\nsql_bot = SQLBot(\n    system=\"You are a sql assistant. You help with SQL queries with one-line answers.\",\n    greeting=\"Hello, I am a SQL assistant. How can I assist you today?\",\n)\n\nprint(sql_bot.send(\"Tell me your preferred SQL operation\"))\n</code></pre> <pre><code>&gt; \"As an SQL assistant, I don't have a preferred SQL operation. I am here to assist with any SQL operation you need help with.\"\n</code></pre>"},{"location":"src/features/chat/advanced-initialization/#next-steps","title":"Next steps","text":"<p>You are welcome to explore our Features section, where you can find the full list of supported features and how to use them.</p>"},{"location":"src/features/chat/chat-memory/","title":"Chat memory","text":"<p>A chat instance holds the conversation history on a property named <code>conversation</code> and uses it across <code>send</code> requests.</p> <p>Here is an example of a chatbot that retains conversation history across multiple <code>send</code> requests. <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n\nsql_bot.send(\"When should I use a LEFT JOIN?\") # (1)!\n&gt; \"You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.\"\n\nsql_bot.send(\"But how is it different from a RIGHT JOIN?\") # (2)!\n&gt; \"A LEFT JOIN retrieves all records from the left table and matching records from the right table, while a RIGHT JOIN retrieves all records from the right table and matching records from the left table.\"\n</code></pre></p> <ol> <li>The first message is sent with only the previously provided context.</li> <li>The second message addresses the previously conducted conversation when generating it's answer.</li> </ol>"},{"location":"src/features/chat/chat-memory/#conversation-history","title":"Conversation History","text":"<p>Let's view the conversation history by accessing the <code>conversation</code> attribute.</p> <pre><code>sql_bot.conversation\n\n&gt; [\n    user: When should I use a LEFT JOIN?, \n    assistant: You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.,\n    user: But how is it different from a RIGHT JOIN?,\n    assistant: A LEFT JOIN retrieves all records from the left table and matching records from the right table, while a RIGHT JOIN retrieves all records from the right table and matching records from the left table.\n]\n</code></pre> <p>Warning</p> <p>Keep in mind that the conversation history does not contain the system prompt. It only contains the user messages and the chatbot responses.</p> <p>If you want to access the system message, you can use the <code>system</code> attribute.</p> <pre><code>sql_bot.system\n\n&gt; \"system: You are a sql assistant. You help with SQL related questions with one-line answers.\\n\"\n</code></pre>"},{"location":"src/features/chat/controlling-chat-behavior/","title":"Controlling chat behavior","text":""},{"location":"src/features/chat/controlling-chat-behavior/#greetings","title":"Greetings","text":"<p>Greetings are used to start the conversation with a bot message instead of a user message. The <code>greeting</code> attribute defines this first message and is added to the conversation on initialization.</p> <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL queries with one-line answers.\n    \"\"\"\n    greeting = \"Hello, I am a SQL assistant. How can I assist you today?\"\n</code></pre> <p>The greeting attribute is later available as a property of the chatbot instance to use when implementing your interface. <pre><code>sql_bot = SQLBot()\nsql_bot.greeting\n\n&gt; \"Hello, I am a SQL assistant. How can I assist you today?\"\n</code></pre></p> <pre><code>sql_bot.send(\"When should I use a LEFT JOIN?\")\n\n&gt; 'You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.'\n\nsql_bot.conversation\n\n&gt; [ # (1)!\n    assistant: Hello, I am a SQL assistant. How can I assist you today?,\n    user: When should I use a LEFT JOIN?,\n    assistant: You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.\n] \n</code></pre> <ol> <li>We can see here that the greeting, initiated by the assistant, is the first message in the conversation.</li> </ol>"},{"location":"src/features/chat/controlling-chat-behavior/#inject-a-message-to-the-memory","title":"Inject a message to the memory","text":"<p>Declarai enables injecting custom messages into the conversation history by using the <code>add_message</code> method.</p> <p>This is super useful when you want to intervene with the conversation flow without necessarily triggering another response from the model.</p> <p>Consider using it for:  </p> <ul> <li>Creating a prefilled conversation even before the user's interaction.  </li> <li>Modifying the chatbot memory after the chatbot has generated a response.  </li> <li>Modifying the chatbot system prompt.</li> <li>Guiding the conversation flow given certain criteria met in the user-bot interaction.</li> </ul> <pre><code>sql_bot = SQLBot()\nsql_bot.add_message(\"From now on, answer I DONT KNOW on any question asked by the user\", role=\"system\") \n# (1)!\nsql_bot.send(\"What is your favorite SQL operation?\")\n\n&gt; \"I don't know.\"\n</code></pre> <ol> <li>The chatbot's conversation history now contains the injected message and reacts accordingly.</li> </ol>"},{"location":"src/features/chat/controlling-chat-behavior/#dynamic-system-prompting","title":"Dynamic system prompting","text":"<p>In the following example, we will pass a parameter to the chatbot system prompt. This value will be populated at runtime and will allow us to easily create base chatbots with varying behaviors.</p> <pre><code>@declarai.experimental.chat\nclass JokeGenerator:\n\"\"\"\n    You are a joke generator. You generate jokes that a {character} would tell.\n    \"\"\" # (1)!\n\n\ngenerator = JokeGenerator()\nfavorite_joke = generator.send(character=\"Spongebob\", message=\"What is your favorite joke?\")\nsquidward_joke = generator.send(message=\"What jokes can you tell about squidward?\")\n\nprint(favorite_joke)\nprint(squidward_joke)\n</code></pre> <ol> <li>The system prompt now contains the parameter <code>{character}</code>. This parameter will be replaced by the value passed to the <code>send</code> method.</li> </ol> <pre><code>&gt; \"Why did the jellyfish go to school? Because it wanted to improve its \"sting-uage\" skills!\"\n&gt; \"Why did Squidward bring a ladder to work? Because he wanted to climb up the corporate \"sour-cules\"!\"\n</code></pre>"},{"location":"src/features/chat/customizing-chat-response/","title":"Customizing the Chat Response","text":"<p>The default response type of the language model messages is <code>str</code>. However, you can overwrite the <code>send</code> method to return a different type. Just like tasks, you can control the type hints by declaring the return type of the <code>send</code> method.</p> <pre><code>from typing import List\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant.\"\"\"\n    ...\n\n    def send(self, operation: str) -&gt; List[str]:\n        ...\n\nsql_bot = SQLBot()\nprint(sql_bot.send(message=\"Offer two sql queries that use the 'SELECT' operation\"))\n&gt; [\n    \"SELECT * FROM table_name;\",\n    \"SELECT column_name FROM table_name;\"\n]\n</code></pre> <p>Warning</p> <p>As with tasks, the message is sent along with the expected return types.  This means that if not careful, a message conflicting with the expected results could cause weird behavior in the llm responses.  For more best-practices, see here.</p>"},{"location":"src/features/chat/debugging-chat/","title":"Debugging Chat","text":"<p>Similarly to debugging tasks, understanding the prompts being sent to the llm is crucial to debugging chatbots. Declarai exposes the <code>compile</code> method for chat instances as well!</p>"},{"location":"src/features/chat/debugging-chat/#compiling-chat","title":"Compiling chat","text":"<p><pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL queries with one-line answers.\n    \"\"\"\n    greeting = \"Hello, I am a SQL assistant. How can I assist you today?\"\n\nsql_bot = SQLBot()\nprint(sql_bot.compile())\n</code></pre> <pre><code>&gt; {\n    'messages': \n        [\n            \"system: You are a sql assistant. You help with SQL queries with one-line answers.\", \n            \"assistant: Hello, I am a SQL assistant. How can I assist you today?\"\n        ]\n}\n</code></pre> Wonderful right? We can view the chatbot's messages in the format they will be sent to the language model.</p>"},{"location":"src/features/evals/","title":"Evaluations","text":"<p>The <code>evals</code> library is an addition over the base <code>declarai</code> library that provides tools to track and benchmark the performance of prompt strategies across models and providers.</p> <p>We understand that a major challenge in the field of prompt engineering is the lack of a standardised way to evaluate along with the continuously evolving nature of the field. As such, we have designed the <code>evals</code> library to be a lean wrapper over the <code>declarai</code> library that allows users to easily track and benchmark changes in prompts and models.</p>"},{"location":"src/features/evals/#usage","title":"Usage","text":"<pre><code>$ python -m declarai.evals.evaluator\nRunning Extraction scenarios...\nsingle_value_extraction... \n---&gt; 100%\nmulti_value_extraction...\n---&gt; 100%\nmulti_value_multi_type_extraction...\n---&gt; 100%\n...\nDone!\n</code></pre>"},{"location":"src/features/evals/#evaluations_1","title":"Evaluations","text":"<p>The output table will allow you to review the performance of your task across models and provides and make an informed decision on which model and provider to use for your task.</p> Provider Model version Scenario runtime output openai gpt-3.5-turbo latest generate_a_poem_no_metadata 1.235s Using LLMs is fun! openai gpt-3.5-turbo 0301 generate_a_poem_no_metadata 0.891s Using LLMs is fun! It's like playing with words Creating models that learn And watching them fly like birds openai gpt-3.5-turbo 0613 generate_a_poem_no_metadata 1.071s Using LLMs is fun! openai gpt-4 latest generate_a_poem_no_metadata 3.494s {'poem': 'Using LLMs, a joyous run,\\nIn the world of AI, under the sun.\\nWith every task, they stun,\\nIndeed, using LLMs is fun!'} openai gpt-4 0613 generate_a_poem_no_metadata 4.992s {'title': 'Using LLMs is fun!', 'poem': \"With LLMs, the fun's just begun, \\nCoding and learning, second to none. \\nComplex tasks become a simple run, \\nOh, the joy when the work is done!\"} openai gpt-3.5-turbo latest generate_a_poem_only_return_type 2.1s Learning with LLMs, a delightful run, Exploring new knowledge, it's never done. With every challenge, we rise and we stun, Using LLMs, the learning is always fun!"},{"location":"src/features/middlewares/","title":"Middlewares","text":"<p>Middlewares are functions that are executed before or after a task is processed.  In Declarai, middlewares are used to extend the functionality of tasks, for simple things like monitoring and logging,  or more complex things like interfering with the task and injecting guardrails into the prompt.</p>"},{"location":"src/features/middlewares/#creating-a-middleware","title":"Creating a middleware","text":"<p>A middleware is a Class that implements a <code>before</code> and/or <code>after</code> method. Each accepts an LLMTask object as an argument, and doesn't return anything.</p> <p>Let's take for example this simple implementation of a logging middleware:</p> <pre><code>import logging\nfrom time import time\n\nfrom declarai.middlewares.base import TaskMiddleware\nfrom declarai.tasks.types import LLMTaskType\n\nlogger = logging.getLogger(\"LLMLogger\")\n\n\nclass LoggingMiddleware(TaskMiddleware):\n    start_time: time = None\n\n    def before(self, _):  # (1)!\n        self.start_time = time()\n\n    def after(self, task: LLMTaskType):  # (2)!\n        end_time = time() - self.start_time\n        log_message = f\"{task.__name__} took {end_time} seconds to complete\"\n        logger.info(log_message)\n        logger.debug(f\"Task results: {task.result}\")\n</code></pre> <ol> <li>We don't need the task object in the <code>before</code> method, so we can ignore it.</li> <li>The after method is called after the task is processed, so we can use the task object to get the task results.</li> </ol> <p>Now all we need to do to use our new middleware is pass it to the <code>@declarai.task</code> decorator:</p> <pre><code>@declarai.task(middlewares=[LoggingMiddleware]) # (1)!\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n</code></pre> <ol> <li>We pass the middleware class to the <code>middlewares</code> argument of the <code>@declarai.task</code> decorator.</li> </ol>"},{"location":"src/integrations/","title":"Integrations","text":"<p>Declarai comes with minimal dependencies out of the box, to keep the core of the library clean and simple. If you would like to extend the functionality of Declarai, you can install one of the following integrations.</p>"},{"location":"src/integrations/#wandb","title":"Wandb","text":"<p>Weights &amp; Biases is a popular tool for tracking machine learning experiments. Recently they have provided an API for their tracking prompts in their platform. The platform has a free tier which you can use to experiment!</p> <pre><code>pip install declarai[wandb]\n</code></pre> <p>Info</p> <p>To use this integration you will need to create an account at wandb. Once created,   you can create a new project and get your API key from the settings page.</p> <p>Once set up, you can use the <code>WandDBMonitorCreator</code> to track your prompts in the platform.</p> <p><pre><code>from typing import Dict\nfrom declarai import Declarai\nfrom declarai.middlewares.third_party.wandb_monitor import WandDBMonitorCreator\n\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\nWandDBMonitor = WandDBMonitorCreator(\n    name=\"&lt;context-name&gt;\",\n    project=\"&lt;project-name&gt;\",\n    key=\"&lt;your-decorators-key&gt;\",\n)\n\n\n@declarai.task(middlewares=[WandDBMonitor])\ndef extract_info(text: str) -&gt; Dict[str, str]:\n\"\"\"\n    Extract the phone number, name and email from the provided text\n    :param text: content to extract the info from\n    :return: The info extracted from the text\n    \"\"\"\n    return Declarai.magic(text=text)\n</code></pre> The tracked prompts should look like this:</p> <p> </p>"},{"location":"src/providers/","title":"Index","text":"<p>Declarai supports the following providers:</p> <ul> <li>OpenAI</li> </ul>"},{"location":"src/providers/openai/","title":"Openai","text":"<p>To use OpenAI models, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? API key <code>DECLARAI_OPENAI_API_KEY</code> <code>Declarai(... openai_token=&lt;api-token&gt;)</code> \u2705"},{"location":"src/providers/openai/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"src/providers/openai/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>DECLARAI_OPENAI_API_KEY</code>. </p> <p>To establish your OpenAI API key as an environment variable, launch your terminal and execute the following command, substituting  with your actual key: <pre><code>export DECLARAI_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This action will maintain the key for the duration of your terminal session. To ensure a longer retention, modify your terminal's settings or corresponding environment files.</p>"}]}